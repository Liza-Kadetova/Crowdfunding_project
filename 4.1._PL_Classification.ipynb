{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание модели для последующего использования\n",
    "\n",
    "Эта модель = модели, обученной в ноутбуке 4_Pl_Classifying.ipynb. Здесь оформлена в класс для сохранения и последующего использования. (С помощью [Алексея](https://github.com/AlexSkrn))\n",
    "\n",
    "**Важно! Для удобства работы этот ноутбук назван и пронумерован в соответствии с другими частями проекта, но при сохранении класса\\модели в формате .py файл должен называться именем, под которым впоследствии будет импортироваться (например, prep.py).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from string import digits\n",
    "import pymorphy2\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.base import BaseEstimator, TransformerMixin # base sklearn classes to inherit methods from, see below\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/liza/PycharmProjects/Planeta_project/plset_fin_upd_clustered.tsv\", sep =\"\\t\")\n",
    "df = df.drop(df.columns[0:2], axis=1)\n",
    "df = df.rename_axis(None, axis=1).rename_axis('Id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В Python можно сохранять все объекты. Для того чтобы сохранить всю модель вместе со способом обработки данных, который использовался для обучения (а по-другому она работать не будет), нужно токенизатор и предиктор упаковать в pipeline. Pipeline - это цепочка процессов обработки данных. У пайплана есть метод fit. После того, как мы зафиттили свои данные, мы сохраняем пайплайн.\n",
    "\n",
    "[Далее - отсюда.](https://towardsdatascience.com/custom-transformers-and-ml-data-pipelines-with-python-20ea2a7adb65)\n",
    "\n",
    "All transformers and estimators in scikit-learn are implemented as Python classes , each with their own attributes and methods. So every time you write Python statements like these:\n",
    "\n",
    "```\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "\n",
    "#Initializing an object of class OneHotEncoder\n",
    "one_hot_enc = OneHotEncoder( sparse = True )\n",
    "\n",
    "#Calling methods on our OneHotEncoder object\n",
    "one_hot_enc.fit( some_data ) #returns nothing\n",
    "transformed_data = one_hot_enc.transform( som_data ) #returns something\n",
    "```\n",
    "\n",
    "you are essentially creating an instance called ‘one_hot_enc’ of the class ‘OneHotEncoder’ using its class constructor and passing it the argument ‘False’ for its parameter ‘sparse’. The OneHotEncoder class has methods such as ‘fit’, ‘transform’ and fit_transform’ and others which can now be called on our instance with the appropriate arguments as seen here.\n",
    "**In order for our custom transformer to be compatible with a scikit-learn pipeline it must be implemented as a class with methods such as fit, transform, fit_transform, get_params, set_params** so we’re going to write all of those…… or we can simply just code the kind of transformation we want our transformer to apply and inherit everything else from some other class!\n",
    "\n",
    "Inheriting from TransformerMixin ensures that all we need to do is write our fit and transform methods and we get fit_transform for free. Inheriting from BaseEstimator ensures we get get_params and set_params for free. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tokenizer', Prep()),\n",
       "                ('vectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=5000,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создаем кастомизированный токенизатор, который можно вставить в Pipeline\n",
    "\n",
    "class Prep(BaseEstimator, TransformerMixin): # we put BaseEstimator and TransformerMixin in parenthesis while declaring the class \n",
    "                                            # to let Python know our class is going to inherit from them.\n",
    "\n",
    "    def __init__(self):\n",
    "        self.morph_analyzer = pymorphy2.MorphAnalyzer()\n",
    "        self.stop_words = stopwords.words('russian')\n",
    "        self.stop_words.extend(['это', '–', '-', 'фонд', 'наш', 'помощь', 'помогать',\n",
    "                   'помочь', 'поддержать', 'поддержка', 'средство', 'который', 'весь',\n",
    "                   'благотворительный', 'деньги', 'рубль', 'год', 'день', 'тысяча',\n",
    "                   'ваш', 'сегодня', 'завтра', 'этот', 'дать', 'проект', 'свой' ])\n",
    "\n",
    "    def prep(self, text):\n",
    "        clean_text = text.translate(str.maketrans('', '', '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~«»№!—'))\n",
    "        clean_text = clean_text.translate(str.maketrans('', '', digits))\n",
    "        clean_text = re.sub(\"-\", \" \", clean_text)\n",
    "        # clean_text = re.sub(\"[a-zA-Z]\", \"\", clean_text)  # исключаем слова латиницей\n",
    "        clean_text = clean_text.lower()\n",
    "        clean_text = clean_text.split()\n",
    "        \n",
    "\n",
    "        # words = [word for word in clean_text if word not in self.stop_words]\n",
    "        # return words\n",
    "\n",
    "        lemmas = [self.morph_analyzer.parse(word)[0].normal_form for word in clean_text]\n",
    "        lemmas = [word for word in lemmas if word not in self.stop_words]\n",
    "        return ' '.join(lemmas)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X = X.map(self.prep)\n",
    "\n",
    "        return X\n",
    "        print(X)\n",
    "\n",
    "# Укладываем все три процесса работы над текстом в пайп\n",
    "pipe = Pipeline([\n",
    "    ('tokenizer', Prep()),\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', LogisticRegression(max_iter=5000))\n",
    "    ]\n",
    "    )\n",
    "\n",
    "# Делим исходные данные\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.Description,\n",
    "                                                    df.Category,\n",
    "                                                    stratify=df.Category)\n",
    "# Обучаем пайп как обычный классификатор\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Дальше предсказывать, оценивать и  сохранять\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "\\\n",
    "⬇︎ Сохраняем обученную модель, которую лальше будем вызывать и использовать для предсказания.\n",
    "[Подробнее в документации sklearn.](https://scikit-learn.org/stable/modules/model_persistence.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pipe.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(pipe, 'pipe.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Важно! Для удобства работы этот ноутбук назван и пронумерован в соответствии с другими частями проекта, но при сохранении класса\\модели в формате .py файл должен называться именем, под которым впоследствии будет импортироваться (например, prep.py).**\n",
    "\n",
    "**Также: датасет, на котором обучалась модель, должен оставаться с тем же названием файла в той же папке, где она лежит.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Основные понятия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference between estimators vs transformers vs predictors in sklearn?\n",
    "\n",
    "While working in Machine Learning projects using scikit-learn library, I would like to highlight important and fundamental concepts that every ML ninja needs to be aware of. In this post i am highlighting few concepts to differentiate estimators vs transformers vs predictors in building machine learning solutions using sklearn.\n",
    "\n",
    "\n",
    "1) **Estimators**: Any objects that can estimate some parameters based on a dataset is called an estimator. The estimation itself is performed by calling fit() method.\n",
    "This method takes one parameter (or two in case of supervised learning algorithms). Any other parameter needed to guide the estimation process is called hyperparameter and must be set as in instance variable.\n",
    "\n",
    "For example: i would like to estimate a mean, median or most frequent value of a column in my dataset.\n",
    "\n",
    "\n",
    "This is a cheat sheet of sklearn estimators. you can find the up to date version here.\n",
    "\n",
    "\n",
    "2) **Transformers**: Transform a dataset. It transforms a dataset by calling transform() method and it returns a transformed dataset. some estimators can also transform a dataset.\n",
    "\n",
    "For example: Imputer class in sklearn is an estimator and a transformer. You can call fit_transform() method that estimate and transform a dataset.\n",
    "\n",
    "Python code: \n",
    "\n",
    "from sklearn.preprocessing inport Imputer\n",
    "\n",
    "imputer = Imputer(strategy=\"mean\") #estimate mean value for dataset columns\n",
    "\n",
    "imputer.fit(mydataset)    # Imputer as an estimator\n",
    "\n",
    "imputer.fit_transform(mydataset)   # Imputer as a transformer and estimator (Combined two steps)\n",
    "\n",
    "3) **Predictors**: making predictions for  given a dataset. A predictor class has predict() method that takes a new instances of a dataset and returns a dataset with corresponding predictions. Also, it contains score() method that measures the quality of the predictions for a giving test dataset.\n",
    "\n",
    "For example: LinearRegression, SVM, Decision Tree,..etc are predictors.\n",
    "\n",
    "\n",
    "**You can combine building blocks of estimators, transformers and predictors as a pipeline in sklearn.** This allows developers to use multiple estimators from a sequence of transformers followed by a final estimator or predictor. This concept is called **composition** in Machine Learning.\n",
    "\n",
    "\n",
    "\n",
    "⬆︎[Отсюда](http://www.mostafa.rocks/2017/04/what-is-difference-between-estimators.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**estimator**: This isn't a word with a rigorous definition but it usually associated with finding a current value in data. If we didn't explicitly count the change in our pocket we might use an estimate. That said, in machine learning it is most frequently used in conjunction with parameter estimation or density estimation. In both cases there is an assumption that data we currently have comes in a form that can be described with a function. With parameter estimation, we believe that the function is a known function that has additional parameters such as rate or mean and we may estimate the value of those parameters. In density estimation we may not even have an assumption about the function but we will attempt to estimate the function regardless. Once we have an estimation we may have at our disposal a model. The estimator then would be the method of generating estimations, for example the method of maximum likelihood.\n",
    "\n",
    "**classifier**: This specifically refers to a type of function (and use of that function) where the response (or range in functional language) is discrete. Compared to this a regressor will have a continuous response. There are additional response types but these are the two most well known. Once we may have built a classifier, it is expected to predict for us from within a finite range of classes which class a vector of data is likely to indicate. As an example a voice recognition software may record a meeting and attempt to record at any given time which of the finite number of meeting attendees are speaking. Building this software we would give each attendee a number that is nominal only and attempt to classify to that number for each segment of speech.\n",
    "\n",
    "**model**: The model is the function (or pooled set of functions) that you may accept or reject as being representative of your phenomenon. The word stems from the idea that you may apply domain knowledge to explaining/predicting the phenomenon though this isn't required. A non-parametric model might be derived entirely from the data at hand but the result is often still called a model. This terminology highlights the fact that what has been constructed when a model has been constructed is not reality but only a 'model' of reality. As George Box has said \"All models are wrong but some are useful\". Having a model allows you to predict but that may not be its purpose; it could also be used to simulate or to explain.\n",
    "\n",
    "⬆︎ [Отсюда](https://stats.stackexchange.com/questions/103475/classifier-vs-model-vs-estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
